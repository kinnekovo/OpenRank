#!/usr/bin/env python3
"""
æµ‹è¯•CSVæ–‡ä»¶ä¸åŠŸèƒ½æ¨¡å—æ•°æ®éœ€æ±‚åŒ¹é…åº¦
éªŒè¯ç”Ÿæˆçš„CSVæ–‡ä»¶æ˜¯å¦æ»¡è¶³5ä¸ªåŠŸèƒ½æ¨¡å—çš„æ•°æ®éœ€æ±‚
"""

import pandas as pd
import os
from datetime import datetime

def test_module_data_coverage():
    """æµ‹è¯•å„æ¨¡å—æ•°æ®éœ€æ±‚è¦†ç›–æƒ…å†µ"""
    print("=== åŠŸèƒ½æ¨¡å—æ•°æ®éœ€æ±‚åŒ¹é…åº¦æµ‹è¯• ===\n")
    
    data_dir = 'data/processed'
    
    # 1. æŠ€æœ¯è¶‹åŠ¿åˆ†ææ¨¡å—éœ€æ±‚æµ‹è¯•
    print("1. æŠ€æœ¯è¶‹åŠ¿åˆ†ææ¨¡å—æ•°æ®éœ€æ±‚éªŒè¯:")
    test_technology_trends(data_dir)
    
    # 2. é¡¹ç›®å½±å“åŠ›åˆ†ææ¨¡å—éœ€æ±‚æµ‹è¯•  
    print("\n2. é¡¹ç›®å½±å“åŠ›åˆ†ææ¨¡å—æ•°æ®éœ€æ±‚éªŒè¯:")
    test_influence_analysis(data_dir)
    
    # 3. è‡ªç„¶è¯­è¨€äº¤äº’æŸ¥è¯¢æ¨¡å—éœ€æ±‚æµ‹è¯•
    print("\n3. è‡ªç„¶è¯­è¨€äº¤äº’æŸ¥è¯¢æ¨¡å—æ•°æ®éœ€æ±‚éªŒè¯:")
    test_nlp_interaction(data_dir)
    
    # 4. æŠ€æœ¯è¶‹åŠ¿é¢„æµ‹æ¨¡å—éœ€æ±‚æµ‹è¯•
    print("\n4. æŠ€æœ¯è¶‹åŠ¿é¢„æµ‹æ¨¡å—æ•°æ®éœ€æ±‚éªŒè¯:")
    test_trend_prediction(data_dir)
    
    # 5. å¯è§†åŒ–ä¸æŠ¥å‘Šç”Ÿæˆæ¨¡å—éœ€æ±‚æµ‹è¯•
    print("\n5. å¯è§†åŒ–ä¸æŠ¥å‘Šç”Ÿæˆæ¨¡å—æ•°æ®éœ€æ±‚éªŒè¯:")
    test_visualization_reports(data_dir)
    
    print("\n=== æ•°æ®è¦†ç›–åº¦æ€»ç»“ ===")

def test_technology_trends(data_dir):
    """æµ‹è¯•æŠ€æœ¯è¶‹åŠ¿åˆ†ææ¨¡å—"""
    print("  ğŸ“Š å¤šç»´åº¦è¶‹åŠ¿å±•ç¤º:")
    
    # è¯­è¨€è¶‹åŠ¿æ•°æ®éªŒè¯
    try:
        lang_df = pd.read_csv(f'{data_dir}/language_trends_detailed.csv')
        print(f"    âœ… è¯­è¨€è¶‹åŠ¿æ•°æ®: {len(lang_df)}æ¡è®°å½•ï¼Œè¦†ç›–{lang_df['language'].nunique()}ç§è¯­è¨€")
        print(f"       - æ—¶é—´èŒƒå›´: {lang_df['date'].min()} åˆ° {lang_df['date'].max()}")
        print(f"       - æ”¯æŒè¶‹åŠ¿æ–¹å‘åˆ†æ: {set(lang_df['trend_direction'])}")
        print(f"       - åŒ…å«æŒ‡æ ‡: {list(lang_df.columns)[2:7]}")
    except Exception as e:
        print(f"    âŒ è¯­è¨€è¶‹åŠ¿æ•°æ®åŠ è½½å¤±è´¥: {e}")
    
    # å…³é”®è¯è¶‹åŠ¿æ•°æ®éªŒè¯
    try:
        keyword_df = pd.read_csv(f'{data_dir}/keyword_trends.csv')
        print(f"    âœ… å…³é”®è¯è¶‹åŠ¿æ•°æ®: {len(keyword_df)}æ¡è®°å½•ï¼Œè¦†ç›–{keyword_df['keyword_category'].nunique()}ä¸ªç±»åˆ«")
        print(f"       - å…³é”®è¯ç±»åˆ«: {list(keyword_df['keyword_category'].unique())}")
        print(f"       - æ”¯æŒç«äº‰åº¦åˆ†æ: {keyword_df['competitiveness'].max():.2f} (æœ€é«˜)")
    except Exception as e:
        print(f"    âŒ å…³é”®è¯è¶‹åŠ¿æ•°æ®åŠ è½½å¤±è´¥: {e}")
    
    # æŠ€æœ¯æˆç†Ÿåº¦æ•°æ®éªŒè¯
    try:
        maturity_df = pd.read_csv(f'{data_dir}/technology_maturity.csv')
        maturity_counts = maturity_df['maturity_stage'].value_counts()
        print(f"    âœ… æŠ€æœ¯æˆç†Ÿåº¦æ•°æ®: {len(maturity_df)}æ¡è®°å½•")
        print(f"       - æˆç†Ÿåº¦åˆ†å¸ƒ: {dict(maturity_counts)}")
        print(f"       - æ”¯æŒé£é™©è¯„ä¼°: {set(maturity_df['risk_level'])}")
    except Exception as e:
        print(f"    âŒ æŠ€æœ¯æˆç†Ÿåº¦æ•°æ®åŠ è½½å¤±è´¥: {e}")

def test_influence_analysis(data_dir):
    """æµ‹è¯•é¡¹ç›®å½±å“åŠ›åˆ†ææ¨¡å—"""
    print("  ğŸ† å½±å“åŠ›åˆ†æ:")
    
    try:
        influence_df = pd.read_csv(f'{data_dir}/influence_ranking.csv')
        print(f"    âœ… å½±å“åŠ›æ’åæ•°æ®: {len(influence_df)}æ¡è®°å½•")
        print(f"       - æ’åç­‰çº§åˆ†å¸ƒ: {dict(influence_df['influence_tier'].value_counts())}")
        print(f"       - æ”¯æŒå¤šç»´è¯„åˆ†: stars({influence_df['stars_score'].max():.1f}), OpenRank({influence_df['openrank_score'].max():.1f}), Activity({influence_df['activity_score'].max():.1f})")
        print(f"       - è¶‹åŠ¿åˆ†ææ”¯æŒ: {set(influence_df['influence_trend'])}")
        print(f"       - ç¤¾åŒºå¥åº·åº¦è¯„ä¼°: {set(influence_df['community_health'])}")
    except Exception as e:
        print(f"    âŒ å½±å“åŠ›æ’åæ•°æ®åŠ è½½å¤±è´¥: {e}")

def test_nlp_interaction(data_dir):
    """æµ‹è¯•è‡ªç„¶è¯­è¨€äº¤äº’æŸ¥è¯¢æ¨¡å—"""
    print("  ğŸ’¬ æ™ºèƒ½é—®ç­”:")
    
    try:
        faq_df = pd.read_csv(f'{data_dir}/faq_dataset.csv')
        print(f"    âœ… FAQæ•°æ®: {len(faq_df)}æ¡è®°å½•")
        print(f"       - è¦†ç›–ç±»åˆ«: {list(faq_df['category'].unique())}")
        print(f"       - å¹³å‡ç½®ä¿¡åº¦: {faq_df['confidence'].mean():.2f}")
        print(f"       - æ”¯æŒå›¾è¡¨å…³è”: {len(faq_df['related_charts'].unique())}ç§")
        print("       - å…¸å‹é—®ç­”ç¤ºä¾‹:")
        for i, row in faq_df.head(3).iterrows():
            print(f"         Q: {row['question']}")
            print(f"         A: {row['answer'][:50]}...")
    except Exception as e:
        print(f"    âŒ FAQæ•°æ®åŠ è½½å¤±è´¥: {e}")

def test_trend_prediction(data_dir):
    """æµ‹è¯•æŠ€æœ¯è¶‹åŠ¿é¢„æµ‹æ¨¡å—"""
    print("  ğŸ”® è¶‹åŠ¿é¢„æµ‹:")
    
    # æŠ€æœ¯æˆç†Ÿåº¦æ•°æ®åŒ…å«é¢„æµ‹ä¿¡æ¯
    try:
        maturity_df = pd.read_csv(f'{data_dir}/technology_maturity.csv')
        print(f"    âœ… é¢„æµ‹æ•°æ®æ”¯æŒ:")
        print(f"       - 6ä¸ªæœˆé¢„æµ‹: {set(maturity_df['prediction_6m'])}")
        print(f"       - 12ä¸ªæœˆé¢„æµ‹: {set(maturity_df['prediction_12m'])}")
        print(f"       - å¢é•¿æ½œåŠ›è¯„ä¼°: {maturity_df['growth_rate'].min():.2f} åˆ° {maturity_df['growth_rate'].max():.2f}")
        print(f"       - é£é™©é¢„è­¦æ”¯æŒ: {len(maturity_df[maturity_df['risk_level'] == 'high'])} ä¸ªé«˜é£é™©æŠ€æœ¯")
    except Exception as e:
        print(f"    âŒ é¢„æµ‹æ•°æ®åŠ è½½å¤±è´¥: {e}")
    
    # å…³é”®è¯è¶‹åŠ¿æ”¯æŒå¸‚åœºé¢„æµ‹
    try:
        keyword_df = pd.read_csv(f'{data_dir}/keyword_trends.csv')
        print(f"    âœ… å¸‚åœºè¶‹åŠ¿åˆ†æ: æ”¯æŒ{keyword_df['keyword_category'].nunique()}ä¸ªæŠ€æœ¯ç±»åˆ«çš„å¸‚åœºé¢„æµ‹")
    except Exception as e:
        print(f"    âŒ å¸‚åœºè¶‹åŠ¿æ•°æ®åŠ è½½å¤±è´¥: {e}")

def test_visualization_reports(data_dir):
    """æµ‹è¯•å¯è§†åŒ–ä¸æŠ¥å‘Šç”Ÿæˆæ¨¡å—"""
    print("  ğŸ“Š ä»ªè¡¨ç›˜æ•°æ®:")
    
    try:
        dashboard_df = pd.read_csv(f'{data_dir}/dashboard_summary.csv')
        print(f"    âœ… ä»ªè¡¨ç›˜æ±‡æ€»: {len(dashboard_df)}é¡¹æ ¸å¿ƒæŒ‡æ ‡")
        print(f"       - æ ¸å¿ƒæŒ‡æ ‡åŒ…æ‹¬:")
        for _, row in dashboard_df.iterrows():
            print(f"         * {row['metric_name']}: {row['metric_value']} ({row['metric_description']})")
    except Exception as e:
        print(f"    âŒ ä»ªè¡¨ç›˜æ•°æ®åŠ è½½å¤±è´¥: {e}")

def analyze_data_quality():
    """åˆ†ææ•°æ®è´¨é‡"""
    print("\n=== æ•°æ®è´¨é‡åˆ†æ ===")
    
    data_dir = 'data/processed'
    csv_files = [
        'language_trends_detailed.csv',
        'keyword_trends.csv', 
        'technology_maturity.csv',
        'influence_ranking.csv',
        'faq_dataset.csv',
        'dashboard_summary.csv'
    ]
    
    total_records = 0
    for file in csv_files:
        try:
            df = pd.read_csv(f'{data_dir}/{file}')
            total_records += len(df)
            print(f"âœ… {file}: {len(df)}æ¡è®°å½•ï¼Œ{len(df.columns)}ä¸ªå­—æ®µ")
        except Exception as e:
            print(f"âŒ {file}: åŠ è½½å¤±è´¥ - {e}")
    
    print(f"\næ€»è®¡æ•°æ®è®°å½•: {total_records}æ¡")
    print(f"å¹³å‡æ¯æ–‡ä»¶: {total_records/len(csv_files):.0f}æ¡è®°å½•")

if __name__ == "__main__":
    test_module_data_coverage()
    analyze_data_quality()